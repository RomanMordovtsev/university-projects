{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCcuv3KRtwZB",
        "outputId": "c72a666b-930e-46a2-c39e-9223e37c0649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMDb dataset already downloaded and extracted.\n",
            "\n",
            "[ K-Means Clustering ]\n",
            "K-Means Accuracy: 0.5345\n",
            "\n",
            "[ Linear Perceptron ]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.85      0.83       969\n",
            "           1       0.85      0.81      0.83      1031\n",
            "\n",
            "    accuracy                           0.83      2000\n",
            "   macro avg       0.83      0.83      0.83      2000\n",
            "weighted avg       0.83      0.83      0.83      2000\n",
            "\n",
            "\n",
            "[ Multi-Layer Perceptron (MLP) ]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       969\n",
            "           1       0.86      0.84      0.85      1031\n",
            "\n",
            "    accuracy                           0.85      2000\n",
            "   macro avg       0.85      0.85      0.85      2000\n",
            "weighted avg       0.85      0.85      0.85      2000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Sentiment Analysis on IMDb Movie Reviews\n",
        "# Algorithms: K-Means, Linear Perceptron, Multi-Layer Perceptron\n",
        "\n",
        "# Step 1: Download and Extract IMDb Dataset\n",
        "import os\n",
        "\n",
        "dataset_url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "dataset_tar = \"aclImdb_v1.tar.gz\"\n",
        "\n",
        "if not os.path.exists('aclImdb'):\n",
        "    print(\"Downloading IMDb dataset...\")\n",
        "    !wget $dataset_url\n",
        "    print(\"Extracting dataset...\")\n",
        "    !tar -xzf $dataset_tar\n",
        "else:\n",
        "    print(\"IMDb dataset already downloaded and extracted.\")\n",
        "\n",
        "# Step 2: Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_files\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 3: Load Data from ./aclImdb/train (Positive & Negative Reviews)\n",
        "dataset = load_files('./aclImdb/train', categories=['pos', 'neg'], shuffle=True)\n",
        "X_raw, y = dataset.data, dataset.target\n",
        "\n",
        "# Subsample for Project Requirement (10,000 samples)\n",
        "X_raw = X_raw[:10000]\n",
        "y = y[:10000]\n",
        "\n",
        "# Step 4: Text Vectorization with TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X = vectorizer.fit_transform([x.decode('utf-8', errors='ignore') for x in X_raw])\n",
        "\n",
        "# Step 5: Split Dataset into Training and Testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Part 1: K-Means Clustering\n",
        "print(\"\\n[ K-Means Clustering ]\")\n",
        "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "kmeans.fit(X_train)\n",
        "\n",
        "from scipy.stats import mode\n",
        "\n",
        "# Map cluster ID to sentiment label (because clusters are unsupervised)\n",
        "cluster_to_label = {}\n",
        "\n",
        "for cluster_id in [0, 1]:\n",
        "    mask = (kmeans.labels_ == cluster_id)\n",
        "    majority_label = mode(np.array(y_train)[mask], keepdims=True).mode[0]\n",
        "    cluster_to_label[cluster_id] = majority_label\n",
        "\n",
        "\n",
        "predicted_clusters = kmeans.predict(X_test)\n",
        "predicted_labels = np.vectorize(cluster_to_label.get)(predicted_clusters)\n",
        "print(f\"K-Means Accuracy: {accuracy_score(y_test, predicted_labels):.4f}\")\n",
        "\n",
        "# Part 2: Linear Perceptron Classifier\n",
        "print(\"\\n[ Linear Perceptron ]\")\n",
        "perceptron = Perceptron(max_iter=1000, random_state=42)\n",
        "perceptron.fit(X_train, y_train)\n",
        "y_pred_perceptron = perceptron.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_perceptron))\n",
        "\n",
        "# Part 3: Multi-Layer Perceptron (MLP) Classifier\n",
        "print(\"\\n[ Multi-Layer Perceptron (MLP) ]\")\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=20, solver='adam', random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "y_pred_mlp = mlp.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_mlp))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2z_jhScmuA3b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}